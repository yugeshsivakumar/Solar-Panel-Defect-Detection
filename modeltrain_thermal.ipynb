{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print('GPU is available.')\n",
    "else:\n",
    "    print('GPU is NOT available. Make sure TensorFlow is installed with GPU support and NVIDIA GPU drivers are installed.')\n",
    "\n",
    "# Define and configure TensorFlow session\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import mimetypes\n",
    "import argparse\n",
    "import imutils\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "image = cv2.imread(\"train\\DJI_0019_R_JPG_jpg.rf.d5882396821ec4d18c77bb8224b526ce.jpg\")\n",
    "cv2.rectangle(image, (226, 73), (317, 129), (0, 255,0), 2)\n",
    "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(image_rgb)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGES_PATH = os.path.sep.join([\"train\"])\n",
    "\n",
    "# Define the path to the CSV file containing annotations for bounding boxes\n",
    "ANNOTS_PATH = os.path.sep.join([\"_annotations.csv\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base output directory\n",
    "BASE_OUTPUT = \"output\"\n",
    "\n",
    "# Create the base output directory if it doesn't exist\n",
    "os.makedirs(BASE_OUTPUT, exist_ok=True)\n",
    "\n",
    "# Define the path to save the trained model\n",
    "MODEL_PATH = os.path.sep.join([BASE_OUTPUT, \"detector.h5\"])\n",
    "\n",
    "# Define the path to save the plot of training history\n",
    "PLOT_PATH = os.path.sep.join([BASE_OUTPUT, \"plot.png\"])\n",
    "\n",
    "# Define the path to save the list of test image filenames\n",
    "TEST_FILENAMES = os.path.sep.join([BASE_OUTPUT, \"test_images.txt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the contents of the annotations CSV file and split it into rows\n",
    "rows = open(ANNOTS_PATH).read().strip().split(\"\\n\")\n",
    "\n",
    "# Initialize lists to store data, targets, and filenames\n",
    "data = []\n",
    "targets = []\n",
    "filenames = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through each row in the annotations data starting from the second row\n",
    "for row in rows[1:]:\n",
    "    # Split the row into individual values\n",
    "    row = row.split(\",\")\n",
    "    # Extract filename, bounding box coordinates (startX, startY, endX, endY)\n",
    "    (filename,width,height,test,xmin,ymin,xmax,ymax) = row\n",
    "    # Construct the path to the image\n",
    "    imagePath = os.path.join(IMAGES_PATH, filename)\n",
    "    # Read the image using OpenCV\n",
    "    image = cv2.imread(imagePath)\n",
    "    # Get the height and width of the image\n",
    "    # Normalize the bounding box coordinates\n",
    "    print(xmin,ymin,xmax,ymax)\n",
    "    xmin = int(xmin) / int(width)\n",
    "    ymin = int(ymin) / int(height)\n",
    "    xmax = int(xmax) / int(width)\n",
    "    ymax = int(ymax) / int(height)\n",
    "    # Load and preprocess the image using Keras\n",
    "    image = load_img(imagePath, target_size=(224, 224))\n",
    "    image = img_to_array(image)\n",
    "    # Append the preprocessed image, target coordinates, and filename to their respective lists\n",
    "    data.append(image)\n",
    "    targets.append((xmin, ymin, xmax, ymax))\n",
    "    filenames.append(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of images to a NumPy array and normalize pixel values\n",
    "data = np.array(data, dtype=\"float32\") / 255.0\n",
    "\n",
    "# Convert the list of targets to a NumPy array\n",
    "targets = np.array(targets, dtype=\"float32\")\n",
    "\n",
    "# Split the data into training and testing sets along with filenames\n",
    "split = train_test_split(data, targets, filenames, test_size=0.10, random_state=42)\n",
    "(trainImages, testImages) = split[:2]\n",
    "(trainTargets, testTargets) = split[2:4]\n",
    "(trainFilenames, testFilenames) = split[4:]\n",
    "\n",
    "# Save the list of testing filenames to a file\n",
    "with open(TEST_FILENAMES, \"w\") as f:\n",
    "    f.write(\"\\n\".join(testFilenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = VGG16(weights=\"imagenet\", include_top=False, input_tensor=Input(shape=(224, 224, 3)))\n",
    "\n",
    "# Freeze the weights of the pretrained VGG16 model\n",
    "vgg.trainable = False\n",
    "\n",
    "# Get the output of the VGG16 model\n",
    "flatten = vgg.output\n",
    "\n",
    "# Flatten the output of the VGG16 model\n",
    "flatten = Flatten()(flatten)\n",
    "\n",
    "# Add fully connected layers for bounding box regression\n",
    "bboxHead = Dense(128, activation=\"relu\")(flatten)\n",
    "bboxHead = Dense(64, activation=\"relu\")(bboxHead)\n",
    "bboxHead = Dense(32, activation=\"relu\")(bboxHead)\n",
    "bboxHead = Dense(4, activation=\"sigmoid\")(bboxHead)\n",
    "\n",
    "# Create a model that takes VGG16's input and predicts bounding box coordinates\n",
    "model = Model(inputs=vgg.input, outputs=bboxHead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the learning rate\n",
    "INIT_LR = 1e-4\n",
    "\n",
    "# Define the number of epochs for training\n",
    "NUM_EPOCHS = 35\n",
    "\n",
    "# Define the batch size for training\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Adam optimizer with the specified learning rate\n",
    "opt = Adam(learning_rate=INIT_LR)\n",
    "\n",
    "# Compile the model with mean squared error loss and the Adam optimizer\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "# Print the summary of the model architecture\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model on the training data with validation on the testing data\n",
    "H = model.fit(\n",
    "    trainImages, trainTargets,\n",
    "    validation_data=(testImages, testTargets),\n",
    "    shuffle=True,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model on the training data with validation on the testing data\n",
    "model.fit(\n",
    "    trainImages, trainTargets,\n",
    "    validation_data=(testImages, testTargets),\n",
    "    shuffle=True,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=20,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('thermal_v1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"thermal_v1.h5\"\n",
    "model = load_model(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the testing image\n",
    "imagePath = r\"Y:\\_Prjs\\solar\\testingfolders\\ourtest\\IMG-20240324-WA0019.jpg\"\n",
    "\n",
    "# Load and preprocess the testing image\n",
    "image = load_img(imagePath, target_size=(224, 224))\n",
    "image = img_to_array(image) / 255.0\n",
    "image = np.expand_dims(image, axis=0)\n",
    "\n",
    "# Make predictions using the trained model\n",
    "preds = model.predict(image)[0]\n",
    "\n",
    "# Extract predicted bounding box coordinates\n",
    "(startX, startY, endX, endY) = preds\n",
    "\n",
    "# Read the original image and resize it\n",
    "image = cv2.imread(imagePath)\n",
    "image = imutils.resize(image, width=600)\n",
    "(h, w) = image.shape[:2]\n",
    "\n",
    "# Scale the predicted coordinates to match the resized image\n",
    "startX = int(startX * w)\n",
    "startY = int(startY * h)\n",
    "endX = int(endX * w)\n",
    "endY = int(endY * h)\n",
    "\n",
    "# Draw the bounding box on the image\n",
    "cv2.rectangle(image, (startX, startY), (endX, endY), (0, 255, 0), 2)\n",
    "\n",
    "# Display the image with the bounding box\n",
    "plt.imshow(image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "solar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
